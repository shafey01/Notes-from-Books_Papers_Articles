## Highlights
 * Page 2 (What Is a Log?): "It is an append-only sequence of records ordered by time"

 * Page 2 (What Is a Log?): "a commit log or journal"

 * Page 3 (Logs in Databases):
   > In fact, the use of logs in much of the rest of this book will be variations on the two uses in database internals:

 * Page 3 (Logs in Databases):
   > 1. The  log  is  used  as  a  publish/subscribe  mechanism  to  transmit  data  to  other replicas

 * Page 4 (Logs in Distributed Systems):
   > 2. The log is used as a consistency mechanism to order the updates that are applied to multiple replicas

 * Page 4 (Logs in Distributed Systems):
   > the state machine replication principle: If two identical, deterministic processes begin in the

 * Page 4 (Logs in Distributed Systems):
   > If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state.

 * Page 4 (Logs in Distributed Systems):
   > The purpose of the log here is to squeeze  all  the  nondeterminism  out  of  the  input  stream  to  ensure  that  each  replica that is processing this input stays in sync.

 * Page 5 (Variety of Log-Centric Designs):
   > this  timestamp  combined with  the  log  uniquely  capture  the  entire  state  of  the  replica.  This  gives  a  discrete, event-driven notion of time that, unlike the machine's local clocks, is easily compara‐ ble between different machines.

 * Page 5 (Variety of Log-Centric Designs):
   > Database  people  gener‐ ally differentiate between physical and logical logging. Physical or row-based logging means logging the contents of each row that is changed. Logical or statement logging means  not  logging  the  changed  rows,  but  instead  logging  the  SQL  commands  that lead to the row changes (the insert, update, and delete statements).

 * Page 5 (Variety of Log-Centric Designs):
   > The state machine model usually refers to an active-active model, where we keep a log of the incoming requests and each replica processes each request in log order.

 * Page 5 (Variety of Log-Centric Designs):
   > called the primary-backup model, is to  elect  one  replica  as  the  leader.

 * Page 6 (An Example):
   > In the primary backup model, a master node is chosen to handle all reads and writes. In the state machine replicatio model, all nodes act as peers.

 * Page 8 (Changelog 101: Tables and Events Are Dual):
   > This  process  works  in  reverse  as  well:  if  you  have  a  table  taking  updates,  you  can record  these  changes  and  publish  a  changelog  of  all  the  updates  to  the  state  of  the table. This changelog is exactly what you need to support near-real-time replicas. In this sense, you can see tables and events as dual: tables support data at rest and logs capture change.

 * Page 8 (Changelog 101: Tables and Events Are Dual):
   > magic of the log is that if it is a complete log of changes, it holds not only the contents of the final version of the table, but can also recreate all other versions that might have existed. It is, effectively, a sort of backup of every previous state of the table.

 * Page 8 (Changelog 101: Tables and Events Are Dual):
   > You interact directly with a checked-out snapshot of the cur‐ rent code, which is analogous to the table. Note that in version control systems, as in other distributed stateful systems, replication happens via the log: when you update, you just pull down the patches and apply them to your current snapshot.

 * Page 11 (Chapter 2. Data Integration):
   > Data integration means making available all the data that an organization has to all the services and systems that need it

 * Page 11 (Chapter 2. Data Integration):
   > The more recognizable term ETL (extract, transform, and load) usually covers only a limited  part  of  data  integration—populating  a  relational  data  warehouse.  However, much  of  what  I  am  describing  can  be  thought  of  as  ETL  that  is  generalized  to  also encompass real-time systems and processing flows.

 * Page 12 (Chapter 2. Data Integration):
   > Once data and processing are available, you can move on to more refined problems such as good data models and consistent, well understood semantics. Finally, concen‐ tration can shift to more sophisticated processing: better visualization, reporting, and algorithmic processing and prediction.

 * Page 12 (Chapter 2. Data Integration):
   > Event data records things that happen rather than things that are. In web systems, this means user activity logging, as well as the machine-level events and sta‐ tistics  required  to  reliably  operate  and  monitor  a  data  center's  worth  of  machines.

 * Page 13 (Log-Structured Data Flow):
   > This data is at the heart of the modern Web: Google's fortune, after all, is generated by a relevance pipeline built on clicks and impressions —that is, events.

 * Page 13 (Log-Structured Data Flow):
   > The  combination  of  more  data  of  more  varieties  and  a  desire  to  get  this  data  into more systems leads to a huge data integration problem.

 * Page 13 (Log-Structured Data Flow):
   > Take  all  of  the  organization's  data  and  put  it  into  a  central  log  for  real-time subscription.

 * Page 13 (Log-Structured Data Flow):
   > Each  subscribing  system  reads  from  this  log  as  quickly  as  it  can, applies each new record to its own store, and advances its position in the log.

 * Page 14 (Log-Structured Data Flow):
   > The log concept gives a logical clock for each change against which all subscribers can be measured. This makes reasoning about the state of the different subscriber systems with respect to one another far simpler, as each has a point in time up to which it has read.

 * Page 14 (Log-Structured Data Flow):
   > Let's say we write a record with log entry X and then need to do a read from the cache. If we want to guarantee that we don't see stale data, we just need to ensure that we don't read from any cache that has not replicated up to X.

 * Page 14 (Log-Structured Data Flow):
   > The log also acts as a buffer that makes data production asynchronous from data con‐ sumption.

 * Page 14 (Log-Structured Data Flow):
   > Neither  the  originating  data  source  nor  the  log  has knowledge  of  the  various  data  destination  systems,  so  consumer  systems  can  be added and removed with no change in the pipeline.

 * Page 15 (Log-Structured Data Flow):
   > the destination system only knows about the log and does not know any details of the system of origin. The consumer system need not concern itself with whether the data came from a relational database, a new-fangled key-value store, or was generated directly by some application. This seems like a minor point, but is, in fact, critical.

 * Page 19 (My Experience at LinkedIn):
   > Amazon has offered a service that is very similar to Kafka called Kinesis. The similarity goes right down to the way partitioning is handled and data  is  retained,  as  well  as  the  fairly  odd  split  in  the  Kafka  API  between  high-  and low-level  consumers.

 * Page 20 (ETL and Organizational Scalability):
   > The data warehouse is meant to be a repository for the clean, integrated data structured to support analysis.

 * Page 20 (ETL and Organizational Scalability):
   > Having this central location that contains  a  clean  copy  of  all  your  data  is  a  hugely  valuable  asset  for  data-intensive analysis and processing.

 * Page 21 (ETL and Organizational Scalability):
   > in the organization. The incentives are not aligned; data producers are often not very aware of the use of the data in the data warehouse and end up creating data that is hard  to  extract  or  requires  heavy,  hard-to-scale  transformation  to  get  it  into  usable form.

 * Page 21 (ETL and Organizational Scalability):
   > The  responsibility  of  integrating  with  this  pipeline  and  providing  a clean, well-structured data feed lies with the producer of this data feed. This means that as part of their system design and implementation, they must consider the prob‐ lem  of  getting  data  out  and  into  a  well-structured  form  for  delivery  to  the  central pipeline.

 * Page 22 (Decoupling Systems):
   > By  contrast,  if  the  organization  had  built  out feeds of uniform, well-structured data, getting any new system full access to all data requires only a single bit of integration plumbing to attach to the pipeline.

 * Page 22 (Decoupling Systems):
   > This architecture also raises a set of different options for where a particular cleanup or transformation can reside: • It  can  be  done  by  the  data  producer  prior  to  adding  the  data  to  the  companywide log. • It can be done as a real-time transformation on the log (which in turn produces a new, transformed log). • It can be done as part of the load process into some destination data system.

 * Page 22 (Decoupling Systems):
   > These details are best handled by the team that creates the data since that team knows the most about its own data. Any logic applied in this stage should be lossless and reversible.

 * Page 24 (Scaling a Log):
   > Distributed  systems  people  often  think  of  a  distributed  log  as  a  slow,  heavyweight abstraction  (and  usually  associate  it  only  with  the  kind  of  metadata  uses  for  which Zookeeper  might  be  appropriate).

 * Page 24 (Scaling a Log):
   > We used a few tricks in Kafka to support this kind of scale: • Partitioning the log • Optimizing throughput by batching reads and writes • Avoiding needless data copies

 * Page 25 (Scaling a Log):
   > the guarantees that we provide are that each partition is order preserving, and Kafka guarantees that appends to a particular partition from a single sender will be delivered in the order they are sent.

 * Page 25 (Scaling a Log):
   > Finally, Kafka uses a simple binary format that is maintained between in-memory log, on-disk log, and in-network data transfers. This allows us to make use of numerous optimizations, including zero-copy data transfer.

 * Page 27 (Chapter 3. Logs and Real-Time Stream Processing):
   > I  see  stream  processing  as  something  much  broader:  infrastructure  for  continuous data processing. I think the computational model can be as general as MapReduce or other distributed processing frameworks, but with the ability to produce low-latency results.

 * Page 29 (Chapter 3. Logs and Real-Time Stream Processing):
   > as Jack Bauer would tell us. When data is collected in batches, it is almost always due to some manual step or lack of digitization, or it is a historical relic left over from the automation of some nondigital process. Transmitting and reacting to data used to be very slow when the mechanics involved transporting pieces of paper and humans did the processing.

 * Page 29 (Chapter 3. Logs and Real-Time Stream Processing):
   > This means that  a  stream  processing  system  produces  output  at  a  user-controlled  frequency instead  of  waiting  for  the  "end"  of  the  data  set  to  be  reached.  In  this  sense,  stream processing is a generalization of batch processing and, given the prevalence of realtime data, a very important generalization.

 * Page 31 (Data Flow Graphs):
   > So what is stream processing? A stream processing job, for our purposes, will be any‐ thing that reads from logs and writes output to logs or other systems. The logs used for input and output connect these processes into a graph of processing stages. Using a  centralized  log  in  this  fashion,  you  can  view  all  the  organization's  data  capture, transformation, and flow as just a series of logs and the processes that read from them and write to them.

 * Page 32 (Logs and Stream Processing):
   > The final use of the log is arguably the most important, and that is to provide buffer‐ ing  and  isolation  to  the  individual  processes.  If  a  processor  produces  output  faster than its downstream consumer can keep up, we have three options: • We  can  block  the  upstream  job  until  the  downstream  job  catches  up  (if  using only TCP and no log, this is what would likely happen). • We can just drop data on the floor. • We can buffer data between the two processes.

 * Page 32 (Logs and Stream Processing):
   > In this complicated web of data processing, if any consumer fails or cannot keep up, upstream producers will block, and blocking will cascade up throughout the data-flow graph, grinding everything to a halt.

 * Page 32 (Logs and Stream Processing):
   > This  leaves  the  only  real  option:  buffering.  The  log  acts  as  a  very,  very  large  buffer that allows the process to be restarted or fail without slowing down other parts of the processing graph.

 * Page 34 (And the Bad...):
   > By "reprocessing," I mean processing input data over again to re-derive output. This is a completely obvious but often ignored requirement. Code will always change. So if you  have  code  that  derives  output  data  from  an  input  stream,  whenever  the  code changes, you will need to recompute your output to see the effect of the change.

 * Page 35 (An Alternative):
   > So how can we do the reprocessing directly from our stream processing job? My pre‐ ferred approach is actually stupidly simple: 1. Use Kafka or some other system that will let you retain the full log of the data you want to be able to reprocess and that allows for multiple subscribers. For exam‐ ple, if you will want to reprocess up to 30 days of data, set your retention in Kafka to 30 days. 2. When  you  want  to  do  the  reprocessing,  start  a  second  instance  of  your  stream processing job that starts processing from the beginning of the retained data, but direct this output data to a new output table. 3. When the second job has caught up, switch the application to read from the new table. 4. Stop the old version of the job and delete the old output table.

 * Page 37 (Stateful Real-Time Processing):
   > The  real  advantage  isn't  about  efficiency  at  all,  but  rather  about  allowing  people  to develop,  test,  debug,  and  operate  their  system  on  top  of  a  single  processing  frame‐ work.

 * Page 37 (Stateful Real-Time Processing):
   > So in cases where simplicity is important, consider this approach as another option in addition to the Lambda Architecture.

 * Page 37 (Stateful Real-Time Processing):
   > The simplest alternative would be to keep state in memory. However, if the process crashed, it would lose its intermediate state. If state is only maintained over a window, the process could just fall back to the point in the log where the window began. How‐ ever, if one is doing a count over an hour, this might not be feasible.

 * Page 37 (Stateful Real-Time Processing):
   > An alternative is to simply store all state in a remote storage system and join over the network  to  that  store.  The  problem  with  this  is  that  there  is  no  locality  of  data  and lots of network round trips.

 * Page 37 (Stateful Real-Time Processing):
   > A  stream  processor  can  keep  its  state  in  a  local  table  or  index:  a  bdb,  RocksDB,  or even something more unusual such as a Lucene or fastbit index. The contents of this store are fed from its input streams (after first perhaps applying arbitrary transforma‐ tion).  It  can  journal  out  a  changelog  for  this  local  index  that  it  keeps  to  allow  it  to restore its state in the event of a crash and restart.

 * Page 38 (Log Compaction):
   > When combined with the logs coming out of databases for data integration purposes, the power of the log/table duality becomes clear. A changelog can be extracted from a database and indexed in different forms by various stream processors to join against event streams.

 * Page 38 (Log Compaction):
   > For keyed data, however, a nice property of a complete log is that you can replay it to recreate  the  state  of  the  source  system.  That  is,  if  I  have  the  log  of  changes,  I  can replay that log into a table in another database and recreate the state of the table at any  point  in  time.  This  also  works  across  different  systems:  you  can  replay  a  log  of updates that originally went into a database into any other type of system that main‐ tains data by primary key (a search index, a local store, and so on).

 * Page 41 (Chapter 4. Building Data Systems with Logs):
   > So there is already one possible simplification in the handling of data in the move to distributed  systems:  coalescing  many  little  instances  of  each  system  into  a  few  big clusters. Many systems aren't good enough to allow this yet because they don't have

 * Page 42 (Unbundling?):
   > security, can't guarantee performance isolation, or just don't scale well enough. How‐ ever, each of these problems is solvable. Instead of running many little single server instances of a system, you can instead run one big multitenant system shared by all the  applications  of  an  entire  organization.  This  allows  for  huge  efficiencies  in  man‐ agement and utilization.

 * Page 42 (Unbundling?):
   > My take is that the explosion of different systems is caused by the difficulty of build‐ ing distributed data systems. By cutting back to a single query type or use case, each system is able to bring its scope down into the set of things that are feasible to build. Running all of these systems yields too much complexity.

 * Page 42 (Unbundling?):
   > The second possibility is that there could be a reconsolidation in which a single sys‐ tem with enough generality starts to merge all the different functions back into a sin‐ gle uber-system. This uber-system could be superficially like the relational database, but its use in an organization would be far different, as you would need only one big system instead of umpteen little ones. In this world, there is no real data integration problem except what is solved inside this system. I think the practical difficulties of building such a system make this unlikely.

 * Page 43 (The Place of the Log in System Architecture):
   > Here are some things a log can do: • Handle data consistency (whether eventual or immediate) by sequencing concur‐ rent updates to nodes • Provide data replication between nodes • Provide  "commit"  semantics  to  the  writer  (such  as  acknowledging  only  when your write is guaranteed not to be lost) • Provide the external data subscription feed from the system • Provide  the  capability  to  restore  failed  replicas  that  lost  their  data  or  bootstrap new replicas • Handle rebalancing of data between nodes

 * Page 46 (Conclusion):
   > This log-oriented data flow has proven to be an enormous simplify‐ ing  assumption.  None  of  these  systems  need  to  have  an  externally  accessible  write API at all; Kafka and databases are used as the system of record, and changes flow to the  appropriate  query  systems  through  that  log.

 * Page 46 (Conclusion):
   > The degree to which these systems rely on the log varies. A fully reliant system could make use of the log for data partitioning, node restore, rebalancing, and all aspects of consistency and data propagation. In this setup, the actual serving tier is nothing less than a sort of cache structured to enable a particular type of processing, with writes going directly to the log.

